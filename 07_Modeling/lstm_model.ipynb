{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf4a789-b354-4205-9d82-a69b3e04bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e429531-1864-4e09-9886-53b9abc807a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title, dataset_name):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    percentages = (cm.T / cm.sum(axis=1) * 100).T\n",
    "    plt.imshow(percentages, interpolation='nearest', cmap=plt.cm.Blues, vmin=0, vmax=100)\n",
    "    plt.title(f'Confusion Matrix - {title} - {dataset_name}')\n",
    "    plt.colorbar(label='Percentage')\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            plt.text(j, i, f\"{cm[i, j]}\\n{percentages[i, j]:.1f}%\", horizontalalignment='center',\n",
    "                     color='white' if percentages[i, j] > 50 else 'black')\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ab93261-5369-4279-a941-dfe752f7bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(data[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_lstm_model(units=50, dropout_rate=0.2, input_shape=(5, 10), num_classes=2):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def modeling(Y, X, nama_model, image=False):\n",
    "    # Stratified split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    for train_index, temp_index in sss.split(X, Y):\n",
    "        X_train, X_temp = X[train_index], X[temp_index]\n",
    "        Y_train, Y_temp = Y[train_index], Y[temp_index]\n",
    "\n",
    "    sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "    for test_index, val_index in sss_val.split(X_temp, Y_temp):\n",
    "        X_test, X_val = X_temp[test_index], X_temp[val_index]\n",
    "        Y_test, Y_val = Y_temp[test_index], Y_temp[val_index]\n",
    "\n",
    "    # Encode labels\n",
    "    num_classes = len(np.unique(Y))\n",
    "    Y_train = to_categorical(Y_train, num_classes=num_classes)\n",
    "    Y_test = to_categorical(Y_test, num_classes=num_classes)\n",
    "    Y_val = to_categorical(Y_val, num_classes=num_classes)\n",
    "\n",
    "    # Check shapes\n",
    "    print(f\"Shape of X_train: {X_train.shape}\")\n",
    "    print(f\"Shape of Y_train: {Y_train.shape}\")\n",
    "    print(f\"Shape of X_val: {X_val.shape}\")\n",
    "    print(f\"Shape of Y_val: {Y_val.shape}\")\n",
    "\n",
    "    # Grid search untuk hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'units': [50, 100],\n",
    "        'dropout_rate': [0.2, 0.3],\n",
    "        'batch_size': [16, 32],\n",
    "        'epochs': [10, 20]\n",
    "    }\n",
    "\n",
    "    def build_model(units, dropout_rate):\n",
    "        return create_lstm_model(units=units, dropout_rate=dropout_rate, input_shape=(X_train.shape[1], X_train.shape[2]), num_classes=num_classes)\n",
    "\n",
    "    model = KerasClassifier(build_fn=build_model, verbose=0)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "    \n",
    "    # Try-except block to capture detailed errors\n",
    "    try:\n",
    "        grid_result = grid.fit(X_train, Y_train)\n",
    "    except Exception as e:\n",
    "        print(f\"GridSearchCV failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Model terbaik\n",
    "    best_model = grid_result.best_estimator_\n",
    "    print(\"Best parameters:\", grid_result.best_params_)\n",
    "\n",
    "    # Evaluasi model\n",
    "    Y_val_pred_prob = best_model.predict_proba(X_val)\n",
    "    Y_val_pred = np.argmax(Y_val_pred_prob, axis=1)\n",
    "    Y_true = np.argmax(Y_val, axis=1)\n",
    "\n",
    "    accuracy_val_pred = accuracy_score(Y_true, Y_val_pred)\n",
    "    precision_val_pred = precision_score(Y_true, Y_val_pred, average='weighted')\n",
    "    recall_val_pred = recall_score(Y_true, Y_val_pred, average='weighted')\n",
    "    f1_micro = f1_score(Y_true, Y_val_pred, average='micro')\n",
    "    f1_macro = f1_score(Y_true, Y_val_pred, average='macro')\n",
    "    cm = confusion_matrix(Y_true, Y_val_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy_val_pred)\n",
    "    print(\"Precision:\", precision_val_pred)\n",
    "    print(\"Recall:\", recall_val_pred)\n",
    "    print(\"F1-Micro:\", f1_micro)\n",
    "    print(\"F1-Macro:\", f1_macro)\n",
    "    \n",
    "    plot_confusion_matrix(cm, classes=np.unique(Y_true), title=nama_model, dataset_name='Validation')\n",
    "\n",
    "    # Save the best model\n",
    "    joblib.dump(best_model, f'best_model_{nama_model}.pkl')\n",
    "\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "840dd6dc-2e45-4cfe-b55b-0af63a44c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def model_utilization(data, sequence_length, nama_model):\n",
    "    # Memisahkan fitur dan label\n",
    "    X = data[:, :-1, :]  # Fitur (misalnya: (n_samples, timesteps, features))\n",
    "    y = data[:, -1, :]   # Label (misalnya: (n_samples, num_classes))\n",
    "\n",
    "    # Mengatur jumlah kelas\n",
    "    num_classes = y.shape[1]\n",
    "    \n",
    "    # Menyusun label menjadi one-hot encoding\n",
    "    y = to_categorical(np.argmax(y, axis=1), num_classes=num_classes)\n",
    "\n",
    "    # Split data menjadi train, validation, dan test\n",
    "    X_train, X_temp, Y_train, Y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Memanggil fungsi modeling\n",
    "    best_model = modeling(Y_train, X_train, nama_model)\n",
    "    \n",
    "    # Evaluasi model pada data val\n",
    "    Y_val_pred_prob = best_model.predict_proba(X_val)\n",
    "    Y_val_pred = np.argmax(Y_val_pred_prob, axis=1)\n",
    "    Y_val_true = np.argmax(Y_val, axis=1)\n",
    "\n",
    "    accuracy_val_pred = accuracy_score(Y_val_true, Y_val_pred)\n",
    "    precision_val_pred = precision_score(Y_val_true, Y_val_pred, average='weighted')\n",
    "    recall_val_pred = recall_score(Y_val_true, Y_val_pred, average='weighted')\n",
    "    f1_micro = f1_score(Y_val_true, Y_val_pred, average='micro')\n",
    "    f1_macro = f1_score(Y_val_true, Y_val_pred, average='macro')\n",
    "    cm = confusion_matrix(Y_val_true, Y_val_pred)\n",
    "\n",
    "    print(\"Validation Accuracy:\", accuracy_val_pred)\n",
    "    print(\"Validation Precision:\", precision_val_pred)\n",
    "    print(\"Validation Recall:\", recall_val_pred)\n",
    "    print(\"Validation F1-Micro:\", f1_micro)\n",
    "    print(\"Validation F1-Macro:\", f1_macro)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    # Save evaluation results to a file\n",
    "    with open(f'evaluation_{nama_model}.txt', 'w') as f:\n",
    "        f.write(f\"Validation Accuracy: {accuracy_val_pred}\\n\")\n",
    "        f.write(f\"Validation Precision: {precision_val_pred}\\n\")\n",
    "        f.write(f\"Validation Recall: {recall_val_pred}\\n\")\n",
    "        f.write(f\"Validation F1-Micro: {f1_micro}\\n\")\n",
    "        f.write(f\"Validation F1-Macro: {f1_macro}\\n\")\n",
    "        f.write(f\"Confusion Matrix:\\n{cm}\\n\")\n",
    "    \n",
    "    # Save the best model\n",
    "    with open(f'best_model_{nama_model}.pkl', 'wb') as file:\n",
    "        joblib.dump(best_model, file)\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "690c27e9-f283-458c-a15f-7eeb9f59d98b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for axis 1 with size 7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 142\u001b[0m\n\u001b[1;32m    140\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m    141\u001b[0m nama_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM_Model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 142\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_utilization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnama_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 132\u001b[0m, in \u001b[0;36mmodel_utilization\u001b[0;34m(file_path, sequence_length, nama_model)\u001b[0m\n\u001b[1;32m    129\u001b[0m X, y \u001b[38;5;241m=\u001b[39m prepare_data(df, sequence_length, feature_columns, target_column)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Modeling and evaluation\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodeling\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnama_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_model\n",
      "Cell \u001b[0;32mIn[26], line 59\u001b[0m, in \u001b[0;36mmodeling\u001b[0;34m(Y, X, nama_model)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# One-hot encoding for labels\u001b[39;00m\n\u001b[1;32m     58\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(Y))\n\u001b[0;32m---> 59\u001b[0m Y_train \u001b[38;5;241m=\u001b[39m \u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m Y_test \u001b[38;5;241m=\u001b[39m to_categorical(Y_test, num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[1;32m     61\u001b[0m Y_val \u001b[38;5;241m=\u001b[39m to_categorical(Y_val, num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/np_utils.py:74\u001b[0m, in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     72\u001b[0m n \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     73\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n, num_classes), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m---> 74\u001b[0m categorical[np\u001b[38;5;241m.\u001b[39marange(n), y] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     75\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m input_shape \u001b[38;5;241m+\u001b[39m (num_classes,)\n\u001b[1;32m     76\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(categorical, output_shape)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 7 is out of bounds for axis 1 with size 7"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import LSTM, Dense, Dropout\n",
    "# import joblib\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# def create_lstm_model(units=50, dropout_rate=0.2, input_shape=(None, 17), num_classes=2):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(units, input_shape=input_shape, return_sequences=True))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(LSTM(units))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(num_classes, activation='softmax'))\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# def prepare_data(df, sequence_length, feature_columns, target_column):\n",
    "#     X, y = [], []\n",
    "#     for i in range(len(df) - sequence_length):\n",
    "#         X.append(df[feature_columns].values[i:i + sequence_length])\n",
    "#         y.append(df[target_column].values[i + sequence_length])\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# def plot_confusion_matrix(cm, classes, title, dataset_name):\n",
    "#     plt.figure(figsize=(8, 8))\n",
    "#     percentages = (cm.T / cm.sum(axis=1) * 100).T\n",
    "#     plt.imshow(percentages, interpolation='nearest', cmap=plt.cm.Blues, vmin=0, vmax=100)\n",
    "#     plt.title(f'Confusion Matrix - {title} - {dataset_name}')\n",
    "#     plt.colorbar(label='Percentage')\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "#     for i in range(len(classes)):\n",
    "#         for j in range(len(classes)):\n",
    "#             plt.text(j, i, f\"{cm[i, j]}\\n{percentages[i, j]:.1f}%\", horizontalalignment='center',\n",
    "#                      color='white' if percentages[i, j] > 50 else 'black')\n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "#     plt.show()\n",
    "#     plt.close()\n",
    "\n",
    "# def build_model(units, dropout_rate, input_shape, num_classes):\n",
    "#     return create_lstm_model(units=units, dropout_rate=dropout_rate, input_shape=input_shape, num_classes=num_classes)\n",
    "\n",
    "# def modeling(Y, X, nama_model):\n",
    "#     # Stratified split\n",
    "#     X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "#     X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42, stratify=Y_temp)\n",
    "\n",
    "#     # One-hot encoding for labels\n",
    "#     num_classes = len(np.unique(Y))\n",
    "#     Y_train = to_categorical(Y_train, num_classes=num_classes)\n",
    "#     Y_test = to_categorical(Y_test, num_classes=num_classes)\n",
    "#     Y_val = to_categorical(Y_val, num_classes=num_classes)\n",
    "\n",
    "#     # Define the Keras model\n",
    "#     model = KerasClassifier(build_fn=build_model, input_shape=(X_train.shape[1], X_train.shape[2]), num_classes=num_classes, verbose=0)\n",
    "\n",
    "#     # Grid search for hyperparameter tuning\n",
    "#     param_grid = {\n",
    "#         'units': [20, 50],  # Range of units to test\n",
    "#         'dropout_rate': [0.2, 0.3],  # Range of dropout rates to test\n",
    "#         'batch_size': [16, 32],  # Range of batch sizes to test\n",
    "#         'epochs': [5, 10]  # Range of epochs to test\n",
    "#     }\n",
    "\n",
    "#     grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=2)  # Use fewer jobs and folds\n",
    "#     grid_result = grid.fit(X_train, Y_train)\n",
    "\n",
    "#     # Best model\n",
    "#     best_model = grid_result.best_estimator_\n",
    "#     print(\"Best parameters:\", grid_result.best_params_)\n",
    "\n",
    "#     # Evaluation\n",
    "#     Y_val_pred_prob = best_model.predict_proba(X_val)\n",
    "#     Y_val_pred = np.argmax(Y_val_pred_prob, axis=1)\n",
    "#     Y_true = np.argmax(Y_val, axis=1)\n",
    "\n",
    "#     accuracy_val_pred = accuracy_score(Y_true, Y_val_pred)\n",
    "#     precision_val_pred = precision_score(Y_true, Y_val_pred, average='weighted')\n",
    "#     recall_val_pred = recall_score(Y_true, Y_val_pred, average='weighted')\n",
    "#     f1_micro_val_pred = f1_score(Y_true, Y_val_pred, average='micro')\n",
    "#     f1_macro_val_pred = f1_score(Y_true, Y_val_pred, average='macro')\n",
    "#     cm = confusion_matrix(Y_true, Y_val_pred)\n",
    "\n",
    "#     print(\"Accuracy:\", accuracy_val_pred)\n",
    "#     print(\"Precision:\", precision_val_pred)\n",
    "#     print(\"Recall:\", recall_val_pred)\n",
    "#     print(\"F1 Micro:\", f1_micro_val_pred)\n",
    "#     print(\"F1 Macro:\", f1_macro_val_pred)\n",
    "    \n",
    "#     # Save the evaluation results\n",
    "#     with open(f'evaluation_{nama_model}.txt', 'w') as f:\n",
    "#         f.write(f\"Accuracy: {accuracy_val_pred}\\n\")\n",
    "#         f.write(f\"Precision: {precision_val_pred}\\n\")\n",
    "#         f.write(f\"Recall: {recall_val_pred}\\n\")\n",
    "#         f.write(f\"F1 Micro: {f1_micro_val_pred}\\n\")\n",
    "#         f.write(f\"F1 Macro: {f1_macro_val_pred}\\n\")\n",
    "\n",
    "#     plot_confusion_matrix(cm, classes=np.unique(Y_true), title=nama_model, dataset_name='Validation')\n",
    "\n",
    "#     # Save the best model\n",
    "#     joblib.dump(best_model, f'best_model_{nama_model}.pkl')\n",
    "\n",
    "#     return best_model\n",
    "\n",
    "# def model_utilization(file_path, sequence_length, nama_model):\n",
    "#     try:\n",
    "#         df = pd.read_csv(file_path, sep=';')\n",
    "#     except pd.errors.ParserError as e:\n",
    "#         print(f\"Error parsing the CSV file: {e}\")\n",
    "#         return None\n",
    "\n",
    "#     # Clean data: remove rows with inconsistent columns\n",
    "#     df = df.dropna()  # Drop rows with any NaN values\n",
    "#     df = df.loc[:, ~df.columns.str.contains('^Unnamed')]  # Drop columns that start with 'Unnamed'\n",
    "\n",
    "#     feature_columns = [f'F{i}' for i in range(1, 18)]\n",
    "#     target_column = 'observation'\n",
    "\n",
    "#     # Prepare data\n",
    "#     X, y = prepare_data(df, sequence_length, feature_columns, target_column)\n",
    "\n",
    "#     # Modeling and evaluation\n",
    "#     best_model = modeling(y, X, nama_model)\n",
    "\n",
    "#     return best_model\n",
    "\n",
    "# # Menjalankan fungsi model_utilization jika dijalankan sebagai script utama\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Contoh path file\n",
    "#     file_path = '../../00_Data_Input/sampel_to_explore.csv'  # Update path here\n",
    "#     sequence_length = 5\n",
    "#     nama_model = 'LSTM_Model'\n",
    "#     best_model = model_utilization(file_path, sequence_length, nama_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51bb5c-9873-453b-8fe5-e10153d985fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_lstm_model(units=50, dropout_rate=0.2, input_shape=(None, 17), num_classes=2):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def prepare_data(df, sequence_length, feature_columns, target_column):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - sequence_length):\n",
    "        X.append(df[feature_columns].iloc[i:i + sequence_length].values)\n",
    "        y.append(df[target_column].iloc[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title, dataset_name):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    percentages = (cm.T / cm.sum(axis=1) * 100).T\n",
    "    plt.imshow(percentages, interpolation='nearest', cmap=plt.cm.Blues, vmin=0, vmax=100)\n",
    "    plt.title(f'Confusion Matrix - {title} - {dataset_name}')\n",
    "    plt.colorbar(label='Percentage')\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            plt.text(j, i, f\"{cm[i, j]}\\n{percentages[i, j]:.1f}%\", horizontalalignment='center',\n",
    "                     color='white' if percentages[i, j] > 50 else 'black')\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def build_model(units, dropout_rate, input_shape, num_classes):\n",
    "    return create_lstm_model(units=units, dropout_rate=dropout_rate, input_shape=input_shape, num_classes=num_classes)\n",
    "\n",
    "def modeling(Y, X, nama_model):\n",
    "    # Adjust labels to start from 0\n",
    "    Y = Y - 1  # Assuming labels start from 1 and need to be adjusted to start from 0\n",
    "\n",
    "    # Stratified split\n",
    "    X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "    X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42, stratify=Y_temp)\n",
    "\n",
    "    # Ensure that X is float32 and Y is int\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_val = X_val.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "    Y_train = Y_train.astype(np.int)\n",
    "    Y_val = Y_val.astype(np.int)\n",
    "    Y_test = Y_test.astype(np.int)\n",
    "\n",
    "    # One-hot encoding for labels\n",
    "    num_classes = len(np.unique(Y))\n",
    "    Y_train = to_categorical(Y_train, num_classes=num_classes)\n",
    "    Y_test = to_categorical(Y_test, num_classes=num_classes)\n",
    "    Y_val = to_categorical(Y_val, num_classes=num_classes)\n",
    "\n",
    "    # Define the Keras model\n",
    "    model = KerasClassifier(build_fn=build_model, input_shape=(X_train.shape[1], X_train.shape[2]), num_classes=num_classes, verbose=0)\n",
    "\n",
    "    # Grid search for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'units': [20, 50],  # Range of units to test\n",
    "        'dropout_rate': [0.2, 0.3],  # Range of dropout rates to test\n",
    "        'batch_size': [16, 32],  # Range of batch sizes to test\n",
    "        'epochs': [5, 10]  # Range of epochs to test\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=2)  # Use fewer jobs and folds\n",
    "    grid_result = grid.fit(X_train, Y_train)\n",
    "\n",
    "    # Best model\n",
    "    best_model = grid_result.best_estimator_\n",
    "    print(\"Best parameters:\", grid_result.best_params_)\n",
    "\n",
    "    # Evaluation\n",
    "    Y_val_pred_prob = best_model.predict_proba(X_val)\n",
    "    Y_val_pred = np.argmax(Y_val_pred_prob, axis=1)\n",
    "    Y_true = np.argmax(Y_val, axis=1)\n",
    "\n",
    "    accuracy_val_pred = accuracy_score(Y_true, Y_val_pred)\n",
    "    precision_val_pred = precision_score(Y_true, Y_val_pred, average='weighted')\n",
    "    recall_val_pred = recall_score(Y_true, Y_val_pred, average='weighted')\n",
    "    f1_micro_val_pred = f1_score(Y_true, Y_val_pred, average='micro')\n",
    "    f1_macro_val_pred = f1_score(Y_true, Y_val_pred, average='macro')\n",
    "    cm = confusion_matrix(Y_true, Y_val_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy_val_pred)\n",
    "    print(\"Precision:\", precision_val_pred)\n",
    "    print(\"Recall:\", recall_val_pred)\n",
    "    print(\"F1 Micro:\", f1_micro_val_pred)\n",
    "    print(\"F1 Macro:\", f1_macro_val_pred)\n",
    "    \n",
    "    # Save the evaluation results\n",
    "    with open(f'evaluation_{nama_model}.txt', 'w') as f:\n",
    "        f.write(f\"Accuracy: {accuracy_val_pred}\\n\")\n",
    "        f.write(f\"Precision: {precision_val_pred}\\n\")\n",
    "        f.write(f\"Recall: {recall_val_pred}\\n\")\n",
    "        f.write(f\"F1 Micro: {f1_micro_val_pred}\\n\")\n",
    "        f.write(f\"F1 Macro: {f1_macro_val_pred}\\n\")\n",
    "\n",
    "    plot_confusion_matrix(cm, classes=np.unique(Y_true), title=nama_model, dataset_name='Validation')\n",
    "\n",
    "    # Save the best model\n",
    "    joblib.dump(best_model, f'best_model_{nama_model}.pkl')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def model_utilization(file_path, sequence_length, nama_model):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep = \";\")\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Error parsing the CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Clean data: remove rows with inconsistent columns\n",
    "    df = df.dropna()  # Drop rows with any NaN values\n",
    "\n",
    "    feature_columns = [f'F{i}' for i in range(1, 18)]\n",
    "    target_column = 'observation'\n",
    "\n",
    "    # Prepare data\n",
    "    X, y = prepare_data(df, sequence_length, feature_columns, target_column)\n",
    "\n",
    "    # Debugging: Check unique values in y\n",
    "    print(f\"Unique values in y: {np.unique(y)}\")\n",
    "\n",
    "    # Modeling and evaluation\n",
    "    best_model = modeling(y, X, nama_model)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "# Menjalankan fungsi model_utilization jika dijalankan sebagai script utama\n",
    "if __name__ == \"__main__\":\n",
    "    # Contoh path file\n",
    "    file_path = '../../00_Data_Input/sampel_to_explore.csv'  # Update path here\n",
    "    sequence_length = 5\n",
    "    nama_model = 'LSTM_Model'\n",
    "    best_model = model_utilization(file_path, sequence_length, nama_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276338ec-c5a2-4b03-b421-c1258dc2fdea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
